# Class 9 - Jan' 27, '24 - Kubernetes

## Introduction
![K8S Logo](https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Kubernetes_logo.svg/2560px-Kubernetes_logo.svg.png)

> Kubernetes is a container orchestrataion engine.

---

### Services

Services provide a single, constant point of entry to a group of Pods providing the same service.

Sometimes, we want Pods to reach the same destination pod on every session, for this we can set the 'sessionAffinity: ClientIP' in the Service spec.

When creating a service with multiple ports, you must specify a name for each port.

You can also give a name to each pod’s port and refer to it by name in the service spec.

```
kind: Pod
spec:
  containers:
  - name: kubia
    ports:
    - name: http
      containerPort: 8080
```

```
apiVersion: v1
kind: Service
spec:
  ports:
  - name: http
    port: 80
    targetPort: http
```

Service discovery:

Kubernetes automatically adds your service host and port information in environment variables as long as your Pod has been created after the service. The name is '\<UPPERCASE_SVC_NAME\>_SERVICE_HOST' & '\<UPPERCASE_SVC_NAME\>_SERVICE_PORT'.

Dashes in the service name are converted to underscores and all letters are uppercased when the service name is used as the prefix in the environment variable’s name.

Each service gets a DNS entry in the internal DNS server, and client pods that know the name of the service can access it through its fully qualified domain name (FQDN) instead of resorting to environment variables.

To revisit the frontend-backend example, a frontend pod can connect to the backend-database service by opening a connection to the following FQDN:

`backend-database.default.svc.cluster.local`

backend-database corresponds to the service name, default stands for the namespace the service is defined in, and svc.cluster.local is a configurable cluster domain suffix used in all cluster local service names.
You can omit the svc.cluster.local suffix and even the namespace, when the frontend pod is in the same namespace as the database pod.

That’s because the service’s cluster IP is a virtual IP, and only has meaning when combined with the service port.

Kubernetes knows when Pods back which Service based on an Endpoints resource. When you create a service with a selector, the endpoints resource is created for you. When you omit the label selector in the service, you can manually create and endpoints resource (with the same name as the service) and have your defined endpoints. The advantage of this is that you can set your endpoints to be public IP addresses, i.e., external services.

To create a service that serves as an alias for an external service, you can create a Service resource with the type field set to ExternalName. This hides the actual service name and its location from pods consuming the service, allowing you to modify the service definition and point it to a different service any time later. ExternalName services are implemented solely at the DNS level—a simple CNAME DNS record is created for the service.

#### Node Port and Load Balancer

Node Port type service is useful to expose your application running in your Pods to external clients.

![Node Port and Load Balancer](https://i.octopus.com/blog/2022-11/difference-clusterip-nodeport-loadbalancer-kubernetes/loadbalancer.png)

Both NodePort and LoadBalancer are types of services in Kubernetes, used to expose applications running in a cluster to external traffic.

NodePort exposes a service on each Node’s IP at a static port (the NodePort). Typically uses ports ranging from 30000-32767. Can be accessed from outside the cluster by `<NodeIP>:<NodePort>`. Ideal for development and small-scale testing. Not recommended for production-level traffic. It automatically creates a ClusterIP service, to which the NodePort service routes.

LoadBalancer Service provisions an external load balancer (if supported by the cloud provider) and assigns a fixed, external IP to the service. Directly integrates with cloud providers’ load balancers (like AWS ELB, Google Cloud Load Balancer). Allows external traffic to access the service through the load balancer’s IP. Suitable for production environments where managing external traffic efficiently is crucial.Takes care of distributing incoming traffic, ensuring high availability and fault tolerance.


**Kubernetes Ingress:**

Ingress is a Kubernetes resource that manages external access to services within a cluster. It provides a way to configure HTTP and HTTPS routing rules, making it possible to expose multiple services through a single entry point. Each Load Balancer service needs an external IP which is expensive whereas Ingress can use 1 external IP and point to many resources.

Ingress resources are configured with rules and backend services, allowing you to specify how incoming traffic should be directed to different services.

An Ingress Controller is a component that runs within a Kubernetes cluster and manages the rules defined in Ingress resources. It interprets the Ingress rules and enforces the traffic routing accordingly.
Popular Ingress Controllers:
   - Nginx Ingress Controller
   - Traefik Ingress Controller
   - HAProxy Ingress Controller
   - Istio Gateway (for advanced features like service mesh)

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
spec:
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: my-service
            port:
              number: 80
```

---

### Next meet:

1. K8S volumes
2. ConfigSets & Secrets
3. Deployments & StatefulSets
4. (maybe) create a Jenkins pipeline to deploy an app to GKE cluster